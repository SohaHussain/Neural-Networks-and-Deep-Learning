Dropout layer helps correcting overfitting of data.
Overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a 
specific combinations of weight, a kind of "conspiracy" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.

This is the idea behind dropout. To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training, making 
it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight
patterns tend to be more robust.

The predictions will no longer be made by one big network, but instead by a committee of smaller networks.

Adding dropout:
keras.Sequential([
    # ...
    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer
    layers.Dense(16),
    # ...
])
