A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also
putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.

Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance).
Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get
"stuck". Consider adding batch normalization to your models, especially if you're having trouble during training.

It seems that batch normalization can be used at almost any point in a network. You can put it after a layer...

layers.Dense(16, activation='relu'),
layers.BatchNormalization(),
... or between a layer and its activation function:

layers.Dense(16),
layers.BatchNormalization(),
layers.Activation('relu'),

And if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn's StandardScaler.
